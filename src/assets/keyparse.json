{
  "transcript": {
    "0.0": "Yeah",
    "4.74": "Mm, yeah",
    "14.74": "So far in this module, we have discussed what a neural network is and how to arrive at the best weights given training data",
    "21.57": "However, we still need to explore more deeply how to train neural networks efficiently",
    "26.59": "This lesson will discuss how to split your data for an unbiased estimate of performance on your neural network model and what insights you can get from observing your neural networks performance on various data splits",
    "39.84": "Let's begin by describing the usual data splits we use to evaluate a machine learning system",
    "46.14": "Let's take as an example, a real life problem were given a data set of 10,000 images of traffic signs with corresponding classification labels",
    "55.62": "We want to train our neural network to perform traffic sign classification",
    "59.94": "How do we approach this problem? Do we train on all the data and then deploy our traffic sign classifier? That approach is guaranteed to fail for the following reasons",
    "69.44": "Given a large enough neural network, we're almost guaranteed to get a very low training loss",
    "74.94": "This is due to the very large number of parameters in a typical deep neural network, allowing it to memorize the training data to a large extent, given a large enough number of training iterations",
    "86.84": "A better approach is to split this data into three parts",
    "91.04": "The training split, the validation split and the testing split",
    "96.04": "As the name suggests, the training split is directly observed by the model during neural network training",
    "101.84": "The loss function is directly minimized on this training set, but as we've stated earlier, we are expecting the value of this function to be very low over the set",
    "111.91": "The validation split is used by developers to test the performance of the neural network when hyper parameters are changed",
    "118.94": "Hyper parameters are those parameters that either modify our network structure or affect our training process but are not a part of the network parameters learned during training",
    "129.84": "Examples include the learning rate, the number of layers, the number of units per layer and the activation function type",
    "137.74": "The final split is called the testing split and is used to get an unbiased estimate of performance",
    "143.94": "The test split should be off limits when developing a neural network architecture, so that the neural network never sees this data during the training or hyper parameter optimization process",
    "154.64": "The only use of the test set should be to evaluate the performance of the final architecture and hyper parameter set before it is deployed on a self driving vehicle",
    "164.14": "Let us now determine what percentage of data goes into each split",
    "168.54": "Before the big data era, it was common to have data sets on the order of thousands of examples",
    "174.1": "In that case, the default percentage of data that goes into each split was approximately 60% for training, 20% for validation and 20% held in reserve for testing",
    "185.6": "However, nowadays it is not uncommon to have data sets on the order of millions of examples",
    "191.77": "Having 20% of the data in the validation and test sets is unnecessary as the validation and test would contain far more samples than are needed for the purpose",
    "202.11": "In such cases, we would find that a training set of 98% of the data, with a validation and test set of 1% of the data each is not uncommon",
    "210.88": "Let us go back to our traffic sign detection problem",
    "213.84": "We assume that our traffic sign data set is comprised of 10,000 labeled examples",
    "219.44": "We can separate our data set into a training, validation and testing split",
    "223.65": "According to the 60 2020 small dataset heuristic",
    "227.44": "We now evaluate the performance of our neural network on each of these splits, using the loss function for a classification problem",
    "234.91": "The loss function is defined as the cross entropy between the prediction and the ground truth labels",
    "241.24": "Cross entropy is strictly greater than zero, so the higher its value, the worst, the performance of our classifier",
    "247.64": "Keep in mind that the neural network only directly observes the training set while the developers use the validation set to determine the best hyper parameters to use",
    "257.14": "The ultimate goal of the training is still minimizing the error on the test set",
    "261.69": "Since it is an unbiased estimate of performance of our system and the data has never been observed by the network",
    "267.98": "Let us first consider the following scenario",
    "270.84": "Let's assume that our estimator gave a cross entropy loss of 0.21 on the training set and a loss of 0.25 on the validation set and finally a loss of 0.3 on the test set",
    "284.4": "Furthermore, due to errors in the labels of the data set, the minimum cross entropy loss that we can expect is 0.18 In this case, we have quite a good classifier as the loss on the three sets are fairly consistent and the loss is close to the minimum achievable loss on the entire task",
    "303.51": "Let's consider a second scenario where the training losses now 1.9 around 10 times that of the minimum loss",
    "311.64": "As we discussed in the previous lesson, we expect any reasonably sized neural network to be able to almost perfectly fit the data given enough training time",
    "320.54": "But in this case, the network was not able to do so",
    "323.34": "We call this scenario where the neural network fails to bring the training loss down under fitting",
    "329.64": "One other scenario we might faces when we have a low training set loss but a high validation and testing set loss",
    "336.86": "For example, we might arrive at the case where the validation losses around 10 times that of the training loss",
    "342.98": "This case is referred to as overfitting and is caused by the neural network optimizing its parameters to precisely reproduce the training data output",
    "351.74": "When we deploy on the validation set, the network cannot generalize well to the new data",
    "356.84": "The gap between training and validation loss is called the generalization gap",
    "362.04": "We want this gap to be as low as possible while still having low enough training loss",
    "367.34": "Let's see how we can try to go from the under fitting or overfitting regime to a good estimator",
    "373.07": "We begin with how to remedy under fitting",
    "375.94": "The first option to remedy under fitting is to train longer if the architecture is suitable for the task at hand",
    "383.05": "Training longer usually leads to a lower training loss",
    "385.99": "If the architecture is too small, training longer might not help",
    "389.53": "In that case, you would want to add more layers to your neural network or add more parameters per layer",
    "395.64": "If both of the above options don't help, your architecture might not be suitable for the task at hand, and you would want to try a different architecture to reduce under fitting",
    "406.04": "Now let's proceed to the most common approaches to reduce overfitting",
    "410.23": "In the case of Overfitting, the easiest thing to do is to just collect more data",
    "414.37": "Unfortunately for self driving cars, collecting training data is very expensive as it requires engineering time for data collection and a tremendous amount of annotator time to properly define the true outputs",
    "426.44": "Another solution for overfitting is regularization",
    "430.64": "Regularization is any modification made to the learning algorithm with an intention to lower the generalization gap but not the training loss",
    "439.6": "If all else fails, the final solution is to revisit the architecture and check if it is suitable for the task at hand",
    "446.74": "In this lesson, we have learned how to interpret the different performance scenarios of our neural network on the training, validation and test splits",
    "455.64": "If it is determined that our network is under fitting, the easiest solution is to train for a longer time or to use a larger neural network",
    "462.99": "However, a much more commonly faced scenario in self driving car perception is overfitting, where a good performance on the trading data does not always translate to good performance on actual roads",
    "475.24": "In the next lesson will focus on how to mitigate the effects of overfitting with various regularization strategies",
    "482.15": "These strategies will allow perception algorithms trained to excel on labeled data sets to continue to work well when driving through the ever changing world around us"
  },
  "keyword": {
    "ever changing world around us": ["490.08"],
    "60 2020 small dataset heuristic": ["224.36"],
    "validation losses around 10 times": ["339.47"],
    "best weights given training data": ["19.65"],
    "data given enough training time": ["318.02"],
    "allow perception algorithms trained": ["483.15"],
    "typical deep neural network": ["77.47"],
    "reasonably sized neural network": ["314.59"],
    "perform traffic sign classification": ["57.55"]
  }
}
